<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>BigJun&#39;s Blog</title>
    <link>http://yoursite.com/</link>
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>记录生活 分享知识</description>
    <pubDate>Mon, 17 Jun 2019 14:11:31 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>toolkit for project</title>
      <link>http://yoursite.com/2019/06/17/toolkit-for-project/</link>
      <guid>http://yoursite.com/2019/06/17/toolkit-for-project/</guid>
      <pubDate>Mon, 17 Jun 2019 08:42:18 GMT</pubDate>
      <description>
      
        
        
          &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
        
      
      </description>
      
      <content:encoded><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><a href="https://blog.csdn.net/qq_15809599/article/details/61239292" target="_blank" rel="noopener">解决git指令更新远程仓库github时每次都要输入用户名和密码问题</a></p><p>导出已有环境：<br>conda env export &gt; environment.yaml </p><p>将environment.yaml文件复制到新主机上之后<br>根据索引文件创建并恢复虚拟环境：<br>conda env create -n your_env_name -f environment.yaml</p><p><a href="https://www.jianshu.com/p/b86c17057da8" target="_blank" rel="noopener">conda环境转移复制和pip包的转移复制</a></p><p><a href="https://blog.csdn.net/qq_35860352/article/details/80685175" target="_blank" rel="noopener">Conda环境移植（克隆）的方法</a></p>]]></content:encoded>
      
      <comments>http://yoursite.com/2019/06/17/toolkit-for-project/#disqus_thread</comments>
    </item>
    
    <item>
      <title>DL_GANs</title>
      <link>http://yoursite.com/2019/05/05/DL-GANs/</link>
      <guid>http://yoursite.com/2019/05/05/DL-GANs/</guid>
      <pubDate>Sun, 05 May 2019 11:56:52 GMT</pubDate>
      <description>
      
        
        
          &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
        
      
      </description>
      
      <content:encoded><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h1><p>The idea behind GANs is that you have two networks, a generator  𝐺  and a discriminator  𝐷 , competing against each other. The generator makes “fake” data to pass to the discriminator. The discriminator also sees real training data and predicts if the data it’s received is real or fake.  </p><blockquote><ul><li>The generator is trained to fool the discriminator, it wants to output data that looks as close as possible to real, training data.   </li><li>The discriminator is a <strong>classifier</strong> that is trained to figure out which data is real and which is fake.</li></ul></blockquote>]]></content:encoded>
      
      <comments>http://yoursite.com/2019/05/05/DL-GANs/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Attention</title>
      <link>http://yoursite.com/2019/04/25/DL_Attention/</link>
      <guid>http://yoursite.com/2019/04/25/DL_Attention/</guid>
      <pubDate>Thu, 25 Apr 2019 08:32:47 GMT</pubDate>
      <description>
      
        
        
          &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
        
      
      </description>
      
      <content:encoded><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script>]]></content:encoded>
      
      <comments>http://yoursite.com/2019/04/25/DL_Attention/#disqus_thread</comments>
    </item>
    
    <item>
      <title>python</title>
      <link>http://yoursite.com/2019/04/24/python/</link>
      <guid>http://yoursite.com/2019/04/24/python/</guid>
      <pubDate>Wed, 24 Apr 2019 12:31:38 GMT</pubDate>
      <description>
      
        
        
          &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
        
      
      </description>
      
      <content:encoded><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h2><h3 id="Numpy数组不能直接做判断"><a href="#Numpy数组不能直接做判断" class="headerlink" title="Numpy数组不能直接做判断"></a>Numpy数组不能直接做判断</h3><ul><li><p>如果直接使用np.ndarray类型变量直接做判断语句（如下，<code>b</code>是否有值），则会报错：<code>The true value of an array with more than one element is ambiguous(具有多个元素的数组的真值是不明确的!)</code></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = tuple([1,2,3]) #type: tuple </span><br><span class="line">b = np.array(a)   #type: ndarray</span><br><span class="line">if a:   #can pass</span><br><span class="line">print(a)   </span><br><span class="line">if b:   #raise error</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure></li><li><p>解决方法：将<code>if b</code>——&gt;<code>if b is not None:</code>或<code>if list(b)/tuple(b):</code>。</p></li></ul><h3 id="argparse模块使用"><a href="#argparse模块使用" class="headerlink" title="argparse模块使用"></a>argparse模块使用</h3><h3 id="使用os模块创建文件夹"><a href="#使用os模块创建文件夹" class="headerlink" title="使用os模块创建文件夹"></a>使用<a href="http://www.runoob.com/python/os-file-methods.html" target="_blank" rel="noopener">os模块</a>创建文件夹</h3><blockquote><p>一般先使用<code>os.path.exists(path)</code>来检查相应文件夹是否存在</p><ol><li>使用<code>os.mkdir(path)</code>创建目录；</li><li>使用<code>os.makedirs(path)</code>递归创建目录；</li></ol></blockquote><pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">if not os.path.exists(path):</span><br><span class="line">os.makedirs(path)</span><br></pre></td></tr></table></figure></code></pre><h3 id="多维数组增加维度"><a href="#多维数组增加维度" class="headerlink" title="多维数组增加维度"></a>多维数组增加维度</h3><blockquote><p>mean_X[:, None]<br>mean_X[:, np.newaxis]<br>mean_X = np.expand_dims(mean_X, axis=1)</p></blockquote><h2 id="Develop"><a href="#Develop" class="headerlink" title="Develop"></a>Develop</h2><h3 id="python包中-init-py"><a href="#python包中-init-py" class="headerlink" title="python包中__init__.py"></a>python包中<code>__init__.py</code></h3><ul><li>将该目录作为Python package的标识；  </li><li>定义package中的<code>_all_</code>,用于模糊导入；   </li><li><a href="https://blog.csdn.net/qq_40794377/article/details/80273668" target="_blank" rel="noopener">传送门</a></li></ul><h3 id="sys与os模块"><a href="#sys与os模块" class="headerlink" title="sys与os模块"></a><a href="https://www.zhihu.com/question/31843617" target="_blank" rel="noopener">sys与os模块</a></h3><blockquote><p>os: This module provides a portable way of using <strong>operating system</strong> dependent functionality.<br>sys: This module provides access to some variables used or maintained by the <strong>interpreter</strong> and to functions that interact strongly with the interpreter.<br>os模块负责程序与操作系统的交互，提供了访问操作系统底层的接口;sys模块负责程序与python解释器的交互，提供了一系列的函数和变量，用于<strong>操控python的运行时环境</strong>。</p></blockquote><h3 id="Python中dist-packages和site-packages区别"><a href="#Python中dist-packages和site-packages区别" class="headerlink" title="Python中dist-packages和site-packages区别"></a><a href="https://blog.csdn.net/huiseguiji1/article/details/45111891" target="_blank" rel="noopener">Python中dist-packages和site-packages区别</a></h3><ul><li>sudo apt-get install 安装的package存放在 /usr/lib/python2.7/dist-packages目录中    </li><li>pip 或者 easy_install安装的package存放在/usr/local/lib/python2.7/dist-packages目录    </li><li>手动从源代码安装的package存放在site-packages目录中;</li></ul><h3 id="命令行运行程序可选项：python-h"><a href="#命令行运行程序可选项：python-h" class="headerlink" title="命令行运行程序可选项：python -h"></a>命令行运行程序可选项：<code>python -h</code></h3><ul><li><code>$ python -E file.py</code>：忽略所有<code>PYTHON*</code>环境变量,如<code>PYTHONPATH</code>;</li><li><code>$ python -m pdb/ipdb file.py</code>：调试模式；</li></ul><h3 id="pdb调试"><a href="#pdb调试" class="headerlink" title="pdb调试"></a>pdb调试</h3><ul><li><a href="https://blog.csdn.net/qingkong1994/article/details/80038199" target="_blank" rel="noopener">常用命令</a></li></ul><h2 id="Common-errors"><a href="#Common-errors" class="headerlink" title="Common errors"></a>Common errors</h2><h3 id="invalid-syntax"><a href="#invalid-syntax" class="headerlink" title="invalid syntax"></a>invalid syntax</h3><ul><li>如果在显示错误处语法正确，请检查前一语句是否漏了括号；</li></ul>]]></content:encoded>
      
      <comments>http://yoursite.com/2019/04/24/python/#disqus_thread</comments>
    </item>
    
    <item>
      <title>posts_supplement</title>
      <link>http://yoursite.com/2019/04/14/1posts-supplement/</link>
      <guid>http://yoursite.com/2019/04/14/1posts-supplement/</guid>
      <pubDate>Sun, 14 Apr 2019 08:40:37 GMT</pubDate>
      <description>
      
        
        
          &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
        
      
      </description>
      
      <content:encoded><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p></p><h2 id="1">Transform learning</h2><img src="/2019/04/14/1posts-supplement/posts-supplement/1.png" alt=""><p></p><p></p><h2 id="2">Convolutional Autoencoder: Transpose convolution</h2><img src="/2019/04/14/1posts-supplement/posts-supplement/2.png" alt=""><p></p><p></p><h2 id="3">Convolutional Autoencoder：Upsampleing + Convolutions</h2><img src="/2019/04/14/1posts-supplement/posts-supplement/3.png" alt=""><p></p>]]></content:encoded>
      
      <comments>http://yoursite.com/2019/04/14/1posts-supplement/#disqus_thread</comments>
    </item>
    
    <item>
      <title>CNNs</title>
      <link>http://yoursite.com/2019/04/06/DL_CNNs/</link>
      <guid>http://yoursite.com/2019/04/06/DL_CNNs/</guid>
      <pubDate>Sat, 06 Apr 2019 07:03:59 GMT</pubDate>
      <description>
      
        
        
          &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
        
      
      </description>
      
      <content:encoded><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><h2 id="Image-Classification-Steps"><a href="#Image-Classification-Steps" class="headerlink" title="Image Classification Steps"></a>Image Classification Steps</h2><ul><li>General Steps<blockquote><p><img src="/2019/04/06/DL_CNNs/CNNs/6.png" alt=""></p></blockquote></li></ul><h2 id="the-advantage-of-CNNs"><a href="#the-advantage-of-CNNs" class="headerlink" title="the advantage of CNNs"></a>the advantage of CNNs</h2><ul><li><p>MLPs &amp;&amp; CNNs：Fully connected &amp;&amp; Local connected<br><img src="/2019/04/06/DL_CNNs/CNNs/10.png" alt=""></p></li><li><p><strong>Local connected/Sparsely connected</strong><br><img src="/2019/04/06/DL_CNNs/CNNs/11.png" alt=""><img src="/2019/04/06/DL_CNNs/CNNs/12.png" alt=""></p></li><li><p><strong>Weights sharing</strong><br><img src="/2019/04/06/DL_CNNs/CNNs/13.png" alt=""></p></li></ul><h2 id="Basic-Concept–Ng"><a href="#Basic-Concept–Ng" class="headerlink" title="Basic Concept–Ng"></a>Basic Concept–Ng</h2><blockquote><p><a href="[http://ai-start.com/dl2017/html/lesson4-week1.html]">kernel、Padding、Strid、Convolution、Pooling</a></p></blockquote><h2 id="the-structure-of-CNNs"><a href="#the-structure-of-CNNs" class="headerlink" title="the structure of CNNs"></a>the structure of CNNs</h2><blockquote><p><img src="/2019/04/06/DL_CNNs/CNNs/9.png" alt=""> </p></blockquote><h2 id="Autoencoders-Encoders-2-Decoders"><a href="#Autoencoders-Encoders-2-Decoders" class="headerlink" title="Autoencoders/Encoders-2-Decoders"></a>Autoencoders/Encoders-2-Decoders</h2><blockquote><p>The key point is to leverage this compressed representation  </p></blockquote><ul><li>Normal images Reconstructuin<blockquote><p><img src="/2019/04/06/DL_CNNs/CNNs/14.png" alt=""></p></blockquote></li><li><p>Denoising Autoencoder</p><blockquote><p><img src="/2019/04/06/DL_CNNs/CNNs/15.png" alt="">  </p></blockquote></li><li><p>Decoder：1) <a href="https://bigjun777.github.io/2019/04/14/posts-supplement/#2" target="_blank" rel="noopener">Transpose Convolution</a>; 2) <a href="https://bigjun777.github.io/2019/04/14/posts-supplement/#3" target="_blank" rel="noopener">Upsampling + Convolutions</a></p></li></ul><h2 id="Transfer-Learning"><a href="#Transfer-Learning" class="headerlink" title="Transfer Learning"></a>Transfer Learning</h2><blockquote><p>Transfer learning involves taking a pre-trained neural network and adapting the neural network to a new, different data set.</p></blockquote><ul><li><p>Four main cases    </p><blockquote><p><img src="/2019/04/06/DL_CNNs/CNNs/19.png" alt="">Take a look <a href="https://bigjun777.github.io/2019/04/14/posts-supplement/#1" target="_blank" rel="noopener">here</a> for more detail.</p></blockquote></li><li><p>Coding part: <strong>Load the pre-trained model, modefied the model as you want, freeze specified parameters if necessary(<code>requires_grad</code>), specify optimiser if necessary</strong>  </p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># Load the pretrained model from pytorch</span><br><span class="line">vgg16 = torchvision.models.vgg16(pretrained=True)</span><br><span class="line"># print out the model structure, see the picture below</span><br><span class="line">print(vgg16)</span><br><span class="line">print(vgg16.classifier[6].in_features) </span><br><span class="line"># Freeze training for all &quot;features&quot; layers</span><br><span class="line">for param in vgg16.features.parameters():</span><br><span class="line">   param.requires_grad = False</span><br><span class="line"># create a new classifer</span><br><span class="line">import torch.nn as nn</span><br><span class="line">n_inputs = vgg16.classifier[6].in_features</span><br><span class="line"># add last linear layer (n_inputs -&gt; 5 flower classes)</span><br><span class="line"># new layers automatically have requires_grad = True</span><br><span class="line">last_layer = nn.Linear(n_inputs, len(classes))</span><br><span class="line">vgg16.classifier[6] = last_layer</span><br><span class="line"># specify optimizer (stochastic gradient descent) and learning rate = 0.001</span><br><span class="line">optimizer = optim.SGD(vgg16.classifier.parameters(), lr=0.001)</span><br></pre></td></tr></table></figure><blockquote><p><img src="/2019/04/06/DL_CNNs/CNNs/20.png" alt=""></p></blockquote><h2 id="Style-Transfer"><a href="#Style-Transfer" class="headerlink" title="Style Transfer"></a>Style Transfer</h2></li></ul><h2 id="Weight-Initialization"><a href="#Weight-Initialization" class="headerlink" title="Weight Initialization"></a><a href="https://github.com/udacity/deep-learning-v2-pytorch/blob/master/weight-initialization/weight_initialization_solution.ipynb" target="_blank" rel="noopener">Weight Initialization</a></h2><blockquote><p>Having good initial weights can place the neural network close to the optimal solution. This allows the neural network to come to the best solution quicker.</p></blockquote><blockquote><p>If every weight is the same(<strong>constant weights</strong>), all the neurons at each layer are producing the same output. This makes it hard to decide which weights to adjust.</p></blockquote><blockquote><p>Commonly, we can use <strong>Uniform Initialization</strong>、<strong>Normal Initialization</strong></p></blockquote><h2 id="CNN中全连接层作用"><a href="#CNN中全连接层作用" class="headerlink" title="CNN中全连接层作用"></a>CNN中全连接层作用</h2><ul><li><a href="https://www.zhihu.com/question/41037974" target="_blank" rel="noopener">全连接层作用</a></li></ul><h1 id="Database"><a href="#Database" class="headerlink" title="Database"></a>Database</h1><h2 id="MNIST"><a href="#MNIST" class="headerlink" title="MNIST"></a>MNIST</h2><ul><li>A hand-written digits dataset:  clean.Centered, heavily pre-processed images<br><img src="/2019/04/06/DL_CNNs/CNNs/1.png" alt=""></li><li>Visualize the data: 28x28 pixels1<br><img src="/2019/04/06/DL_CNNs/CNNs/2.png" alt=""></li></ul><h2 id="CIFAR-10"><a href="#CIFAR-10" class="headerlink" title="CIFAR-10"></a>CIFAR-10</h2><ul><li>Small color images that fall into one of ten classes:60000 images(32x32)<br><img src="/2019/04/06/DL_CNNs/CNNs/8.png" alt=""></li></ul><h2 id="ImageNet"><a href="#ImageNet" class="headerlink" title="ImageNet"></a>ImageNet</h2><h1 id="Data-processsing"><a href="#Data-processsing" class="headerlink" title="Data processsing"></a>Data processsing</h1><h2 id="Data-Normalization"><a href="#Data-Normalization" class="headerlink" title="Data Normalization"></a>Data Normalization</h2><ul><li>basic<br><img src="/2019/04/06/DL_CNNs/CNNs/3.png" alt=""></li></ul><h2 id="Data-flattened"><a href="#Data-flattened" class="headerlink" title="Data flattened"></a>Data flattened</h2><ul><li>To input the data into <strong>MLPs(Multi-Layer Perceptrons)</strong>,you need to convert a maticx to a vector;<br><img src="/2019/04/06/DL_CNNs/CNNs/4.png" alt=""><img src="/2019/04/06/DL_CNNs/CNNs/5.png" alt=""></li></ul><h2 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a><a href="https://pytorch.org/docs/stable/torchvision/transforms.html" target="_blank" rel="noopener">Data Augmentation</a></h2><ul><li><p>To deal with:</p><blockquote><p>Scale Invariance;Rotation Invariance;Translation Invariance.</p></blockquote>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># convert data to a normalized torch.FloatTensor</span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.RandomHorizontalFlip(), # randomly flip and rotate</span><br><span class="line">    transforms.RandomRotation(10),</span><br><span class="line">    transforms.ToTensor(),  # Convert a PIL Image or numpy.ndarray to tensor.</span><br><span class="line">    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure></li></ul><h1 id="Coding-Part"><a href="#Coding-Part" class="headerlink" title="Coding Part"></a>Coding Part</h1><h2 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h2><ul><li><p><a href="https://pytorch.org/docs/stable/torch.html#module-torch" target="_blank" rel="noopener">Pytorch Package</a>    </p><blockquote><p> Pytorch is Python package that provides two high-level features:   </p><ol><li>Tensor computation (like NumPy) with strong GPU acceleration   </li><li>Deep neural networks built on a tape-based autograd system</li></ol></blockquote></li><li><p><a href="https://pytorch.org/docs/stable/torchvision/index.html" target="_blank" rel="noopener">torchvision</a>    </p><blockquote><p>The <code>torchvision</code> package consists of <strong>pupular datasets、model architectures、and common image transformations</strong> for computer vision;</p></blockquote></li><li><p>Import necessary libraries for working with data and Pytorch</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">import numpy as np</span><br><span class="line">from torchvision import datasets</span><br><span class="line">import torchvision.transforms as transforms</span><br></pre></td></tr></table></figure></li></ul><h2 id="Load-the-Data"><a href="#Load-the-Data" class="headerlink" title="Load the Data"></a>Load the Data</h2><ul><li><p>Common database: <code>torchvision.transforms.</code>、<code>torchvision.datasets.</code>、<code>torch.utils.data.Dataloader</code></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.ToTensor()  or  transforms.Compose([... ,...])</span><br><span class="line">train_data = torchvision.datasets.MNIST(root=&apos;data&apos;, train=True, download=True, transform=transform)</span><br><span class="line">test_data = torchvision.datasets.MNIST(root=&apos;data&apos;, train=False, download=True, transform=transform)</span><br><span class="line">train_loader = torch.utils.data.Dataloader(train_data, batch_size=20, num_workers=0)</span><br><span class="line">test_loader = torch.utils.data.Dataloader(test_data, batch_size=20, num_workers=0)</span><br></pre></td></tr></table></figure></li><li><p>You can define a new Imagedatas class to load data from a directory: like <a href="https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder" target="_blank" rel="noopener">ImageFolder</a></p><blockquote><p><img src="/2019/04/06/DL_CNNs/CNNs/17.png" alt=""></p></blockquote></li></ul><h2 id="Visualize-the-Data"><a href="#Visualize-the-Data" class="headerlink" title="Visualize the Data"></a>Visualize the Data</h2><ul><li><p>Gray：<code>matplotlib.pyplot</code>、<a href="https://matplotlib.org/api/_as_gen/matplotlib.figure.Figure.html" target="_blank" rel="noopener">plt.figure()</a>、<a href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html" target="_blank" rel="noopener">plt.imshow()</a>   </p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">dataiter = iter(train_loader)</span><br><span class="line">images, labels = dataiter.next()</span><br><span class="line">images = images.numpy()# from torch to numpy</span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(25,4))</span><br><span class="line">for idx in np.arange(20):</span><br><span class="line">ax = fig.add_subplot(2, 20/2, id+1, xticks=[], yticks=[])</span><br><span class="line">ax.imshow(np.squeeze(images[idx], cmap=&apos;gray&apos;))</span><br><span class="line">ax.set_title(str(labels[idx].item))</span><br></pre></td></tr></table></figure></li><li><p>RGB</p><blockquote><p><img src="/2019/04/06/DL_CNNs/CNNs/18.png" alt=""></p></blockquote></li></ul><ul><li>Single image annotation    <blockquote><p><a href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.annotate.html" target="_blank" rel="noopener">matplotlib.pyplot.annotate</a>、<a href="https://blog.csdn.net/wizardforcel/article/details/54782628" target="_blank" rel="noopener">Matplotlib 中文用户指南 4.5 标注</a><img src="/2019/04/06/DL_CNNs/CNNs/16.png" alt=""></p></blockquote></li></ul><h2 id="Define-the-Network-Architecture"><a href="#Define-the-Network-Architecture" class="headerlink" title="Define the Network Architecture"></a>Define the Network <a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">Architecture</a></h2><ul><li><p><a href="https://pytorch.org/docs/stable/nn.html?highlight=linear#torch.nn.Linear" target="_blank" rel="noopener">Linear layers</a></p><blockquote><p>torch.nn.Linear(in_features, out_features, bias=True)   </p><blockquote><p>Input: (N, *, in_features)(N,∗,in_features) where \∗ means any number of additional dimensions.</p></blockquote></blockquote></li><li><p><a href="[https://pytorch.org/docs/stable/nn.html#conv2d]">Convolutional Layers</a>    </p><blockquote><p><img src="/2019/04/06/DL_CNNs/CNNs/7.png" alt=""></p></blockquote></li><li><p>[activation、dropout function][f]</p></li></ul><h2 id="Specify-Loss-Function-and-Optimizeroptim"><a href="#Specify-Loss-Function-and-Optimizeroptim" class="headerlink" title="Specify Loss Function and Optimizeroptim"></a>Specify <a href="https://pytorch.org/docs/stable/nn.html#loss-functions" target="_blank" rel="noopener">Loss Function</a> and Optimizer<a href="https://pytorch.org/docs/stable/optim.html" target="_blank" rel="noopener">optim</a></h2><ul><li>Common：just use existed class: like <code>nn.CrossEntropyLoss()</code>. You can also define your own Loss function, it is commonly defined as a class.     <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)</span><br></pre></td></tr></table></figure></li></ul><h2 id="Train-the-Network"><a href="#Train-the-Network" class="headerlink" title="Train the Network"></a>Train the Network</h2><blockquote><p>The steps for training/learning from a batch of data are described in the comments below:</p><pre><code>1. Clear the gradients of all optimized variables.2. Forward pass: compute predicted outputs by passing inputs to the model.3. Calculate the loss.4. Backward pass: compute gradient of the loss with respect to model parameters.5. Perform a single optimization step(parameter update).6. Updata averge training loss.</code></pre></blockquote><ul><li>Basic process：prep model for training <code>model.train()</code> 、<code>model.eval()</code>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">for data, target in train_loader:</span><br><span class="line">    # clear the gradients of all optimized variables</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    # forward pass: compute predicted outputs by passing inputs to the model</span><br><span class="line">    output = model(data)</span><br><span class="line">    # calculate the loss</span><br><span class="line">    loss = criterion(output, target)</span><br><span class="line">    # backward pass: compute gradient of the loss with respect to model parameters</span><br><span class="line">    loss.backward()</span><br><span class="line">    # perform a single optimization step (parameter update)</span><br><span class="line">    optimizer.step()</span><br><span class="line">    # update running training loss</span><br><span class="line">    train_loss += loss.item()*data.size(0)</span><br></pre></td></tr></table></figure></li></ul>]]></content:encoded>
      
      <comments>http://yoursite.com/2019/04/06/DL_CNNs/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Notes of Manipulation class</title>
      <link>http://yoursite.com/2019/03/25/Notes-of-Manipulation-class/</link>
      <guid>http://yoursite.com/2019/03/25/Notes-of-Manipulation-class/</guid>
      <pubDate>Mon, 25 Mar 2019 12:30:01 GMT</pubDate>
      <description>
      
        
        
          &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
        
      
      </description>
      
      <content:encoded><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Matlab-Coding"><a href="#Matlab-Coding" class="headerlink" title="Matlab Coding"></a>Matlab Coding</h1><h2 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h2><ul><li><code>fprintf(&#39;...&#39;)</code>：输出字符；</li></ul><h2 id="提取数组、矩阵中元素"><a href="#提取数组、矩阵中元素" class="headerlink" title="提取数组、矩阵中元素"></a>提取数组、矩阵中元素</h2><pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ a = A(1); b = B(:,1)</span><br></pre></td></tr></table></figure></code></pre><h2 id="矩阵运算：点乘、逆、转置、行列式"><a href="#矩阵运算：点乘、逆、转置、行列式" class="headerlink" title="矩阵运算：点乘、逆、转置、行列式"></a>矩阵运算：点乘、逆、转置、行列式</h2><pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ans = dot(a,b)</span><br><span class="line">$ ans = inv(T)</span><br><span class="line">$ ans = T&apos;</span><br><span class="line">$ ans = det(T)</span><br></pre></td></tr></table></figure></code></pre><h2 id="绘图"><a href="#绘图" class="headerlink" title="绘图"></a>绘图</h2><ul><li><code>plot(x[,y][,&#39;r&#39;])</code>：直接绘制图像；   </li><li><code>subplot(221)</code>: 子窗口绘制图像；   </li><li><code>title(&#39;helloworld&#39;)</code>: 设置标题；   </li><li><code>legend(&#39;1&#39;,&#39;2&#39;,&#39;3&#39;)</code>: 按绘制顺序给图像标注；  </li><li><code>axis equal; axis([0 5 0 5])</code>: 设置轴刻度范围</li><li><code>figure(&#39;NumberTitle&#39;, &#39;off&#39;, &#39;Name&#39;, &#39;abc&#39;)</code>：隐藏标题号码，设置标题；</li></ul><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ul><li><code>tic, ...(执行程序), toc</code>：计算程序执行时间；  </li><li><code>edit file.m</code>：打开文件；</li></ul><h1 id="2DPose"><a href="#2DPose" class="headerlink" title="2DPose"></a>2DPose</h1><h2 id="Positon-and-Pose"><a href="#Positon-and-Pose" class="headerlink" title="Positon and Pose"></a>Positon and Pose</h2><ul><li>How to describe a point and pose? <img src="/2019/03/25/Notes-of-Manipulation-class/1.png" alt=""></li></ul><h2 id="Relative-Positions"><a href="#Relative-Positions" class="headerlink" title="Relative Positions"></a>Relative Positions</h2><ul><li>How to transform vector from one frame to another?<img src="/2019/03/25/Notes-of-Manipulation-class/2.png" alt=""></li></ul><h2 id="Relative-Poses"><a href="#Relative-Poses" class="headerlink" title="Relative Poses"></a>Relative Poses</h2><ul><li><p>How to calculate the relative points or poses?<img src="/2019/03/25/Notes-of-Manipulation-class/3.png" alt=""><img src="/2019/03/25/Notes-of-Manipulation-class/4.png" alt=""></p></li><li><p>Pose algebra<img src="/2019/03/25/Notes-of-Manipulation-class/5.png" alt=""></p></li></ul><h2 id="Describing-rotation"><a href="#Describing-rotation" class="headerlink" title="Describing rotation"></a>Describing rotation</h2><ul><li><p>How to calculate the rotation?<img src="/2019/03/25/Notes-of-Manipulation-class/6.png" alt=""><img src="/2019/03/25/Notes-of-Manipulation-class/7.png" alt=""></p></li><li><p>Rotation matix<img src="/2019/03/25/Notes-of-Manipulation-class/8.png" alt=""><img src="/2019/03/25/Notes-of-Manipulation-class/9.png" alt=""></p></li></ul><h2 id="Describing-rotation-and-translation"><a href="#Describing-rotation-and-translation" class="headerlink" title="Describing rotation and translation"></a>Describing rotation and translation</h2><ul><li>Homogeneous transform<img src="/2019/03/25/Notes-of-Manipulation-class/10.png" alt=""><img src="/2019/03/25/Notes-of-Manipulation-class/11.png" alt=""><img src="/2019/03/25/Notes-of-Manipulation-class/12.png" alt=""></li></ul><h2 id="Coding-Part"><a href="#Coding-Part" class="headerlink" title="Coding Part"></a>Coding Part</h2><ul><li><p><code>rot2()</code>：2 dimensional rotation matrix</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rot2(0.2) </span><br><span class="line">rot2(30, &apos;deg&apos;)</span><br></pre></td></tr></table></figure><p>  <img src="/2019/03/25/Notes-of-Manipulation-class/19.png" alt=""></p></li><li><p><code>trot2()</code>：homogeneous 2 dimensional rotation matrix</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trot2(30, &apos;deg&apos;)</span><br></pre></td></tr></table></figure><p>  <img src="/2019/03/25/Notes-of-Manipulation-class/20.png" alt=""></p></li><li><p><code>transl2()</code>： homogeneous transformation representing puretranslation</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transl2(x,y)</span><br></pre></td></tr></table></figure><p>  <img src="/2019/03/25/Notes-of-Manipulation-class/22.png" alt=""></p></li><li><p><code>se2()</code>： homogeneous transform matrix: providing the translation in the x and y directions as well as the angle to be rotated</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">se2(x,y,angle,&apos;deg&apos;)/se2(x,y,radian)</span><br></pre></td></tr></table></figure><p>  <img src="/2019/03/25/Notes-of-Manipulation-class/21.png" alt="">    </p><blockquote><p>actually: <code>se2(x,y,angle,&#39;deg&#39;) == transl2(x,y) * trot2(angle,&#39;deg&#39;)</code></p></blockquote></li><li><p><code>e2h() &amp;&amp; h2e()</code>： function e2h converts Euclid-ean coordinates to homogeneous and h2e performs the inverse conversion;</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ p1 = h2e(inv(T1) * e2h(P)) % P = [1;2]</span><br></pre></td></tr></table></figure><blockquote><p>More compactly this can be written as</p><blockquote><p>p1 = homtrans( inv(T1), P)</p></blockquote></blockquote></li><li><p>Example1: <code>pose compounding is not commutative(交换的)</code></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">T1 = se2(1,2,30,&apos;deg&apos;)</span><br><span class="line">T2 = transl2(2,1) * trot2(0)</span><br><span class="line">T3 = T1 * T2    %This can be thought of the pose 2 with respect to the frame 1</span><br><span class="line">T4 = T2 * T1</span><br></pre></td></tr></table></figure><p>  <img src="/2019/03/25/Notes-of-Manipulation-class/23.png" alt=""></p></li><li><p>Example2：point with respace to 1</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">p = [3 2]&apos;</span><br><span class="line">plot_point(p)</span><br><span class="line">p1 = inv(T1) * [p;1] % = p1 = inv(T1) * e2h(p)</span><br></pre></td></tr></table></figure><p>  <img src="/2019/03/25/Notes-of-Manipulation-class/24.png" alt=""></p></li></ul><h1 id="3DPose"><a href="#3DPose" class="headerlink" title="3DPose"></a>3DPose</h1><h2 id="Basic-concept"><a href="#Basic-concept" class="headerlink" title="Basic concept"></a>Basic concept</h2><ul><li><p>Point &amp;&amp; Pose<img src="/2019/03/25/Notes-of-Manipulation-class/14.png" alt=""><img src="/2019/03/25/Notes-of-Manipulation-class/15.png" alt=""></p></li><li><p>The Right-Hand Rule</p><blockquote><p>Angles increase positively in the anti-clockwise direction</p></blockquote><p>  <img src="/2019/03/25/Notes-of-Manipulation-class/13.png" alt=""></p><h2 id="Relative-Poses-1"><a href="#Relative-Poses-1" class="headerlink" title="Relative Poses"></a>Relative Poses</h2></li><li><p>Relative position/pose<img src="/2019/03/25/Notes-of-Manipulation-class/16.png" alt=""></p></li><li><p>Pose algebra<img src="/2019/03/25/Notes-of-Manipulation-class/17.png" alt=""><img src="/2019/03/25/Notes-of-Manipulation-class/18.png" alt=""></p></li></ul><h2 id="Representing-Orientation-in-3-Dimensions"><a href="#Representing-Orientation-in-3-Dimensions" class="headerlink" title="Representing Orientation in 3-Dimensions"></a>Representing Orientation in 3-Dimensions</h2><h3 id="Orthonormal-Rotation-Matrix"><a href="#Orthonormal-Rotation-Matrix" class="headerlink" title="Orthonormal Rotation Matrix"></a>Orthonormal Rotation Matrix</h3><ul><li><p>3×3 orthonormal matrix<br><img src="/2019/03/25/Notes-of-Manipulation-class/25.png" alt=""><img src="/2019/03/25/Notes-of-Manipulation-class/26.png" alt=""><img src="/2019/03/25/Notes-of-Manipulation-class/61.png" alt=""></p></li><li><p>Coding part</p><blockquote><p><code>rotx()</code>：3 dimensional rotation matrix</p></blockquote>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ R = rotx(pi/2)</span><br><span class="line">$ R = rotx(45, &apos;deg&apos;)</span><br></pre></td></tr></table></figure><blockquote><p><code>trplot() &amp;&amp; tranimate()</code>：display a 3d transformation</p></blockquote>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ trplot(R)</span><br><span class="line">$ tranimate(R)</span><br></pre></td></tr></table></figure></li></ul><h3 id="Three-Angle-Representations"><a href="#Three-Angle-Representations" class="headerlink" title="Three-Angle Representations"></a>Three-Angle Representations</h3><ul><li><p>Rotation sequences<br><img src="/2019/03/25/Notes-of-Manipulation-class/28.png" alt=""></p></li><li><p>Euler angles：Euler’s rotation theorem requires successive rotation about three axes such that notwo successive rotations are about the same axis.   </p><blockquote><p><strong>Representation</strong></p><blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/27.png" alt=""></p></blockquote></blockquote><blockquote><p><strong>Coding</strong>:</p><blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/29.png" alt=""></p></blockquote></blockquote><blockquote><p><strong>if θ is negative</strong></p><blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/30.png" alt=""></p></blockquote></blockquote><blockquote><p><strong>if θ=0</strong></p><blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/31.png" alt=""></p></blockquote></blockquote></li><li><p>Cardan angles/Roll-Pitch-Yaw angles</p><blockquote><p><strong>Representation</strong></p><blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/32.png" alt=""></p></blockquote></blockquote><blockquote><p><strong>Coding</strong></p><blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/33.png" alt=""></p></blockquote></blockquote></li><li>Fundamental problem：Singularities and Gimbal Lock<blockquote><p>This occurs when the rotational axis of the middle term in the sequence becomes parallel to the rotation axis of the first or third term. </p><blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/34.png" alt=""></p></blockquote></blockquote></li></ul><h3 id="Two-Vector-Representation"><a href="#Two-Vector-Representation" class="headerlink" title="Two Vector Representation"></a>Two Vector Representation</h3><ul><li>Two Vector Representation<blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/35.png" alt=""></p></blockquote></li></ul><h3 id="Rotation-about-an-Arbitrary-Vector"><a href="#Rotation-about-an-Arbitrary-Vector" class="headerlink" title="Rotation about an Arbitrary Vector"></a>Rotation about an Arbitrary Vector</h3><ul><li><p>Any two independent orthonormal coordinate frames can be related by a single rotation about some axis.</p><blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/47.png" alt=""></p></blockquote></li><li><p>Finding the axis;</p><blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/48.png" alt=""></p></blockquote></li><li><p>Coding part: <code>tr2angvec(R)、eig(R)、angvec2r(pi/2, [1 0 0])</code></p><blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/49.png" alt=""><br><img src="/2019/03/25/Notes-of-Manipulation-class/50.png" alt=""><br><img src="/2019/03/25/Notes-of-Manipulation-class/51.png" alt=""></p></blockquote></li></ul><h3 id="Quaternion"><a href="#Quaternion" class="headerlink" title="Quaternion"></a>Quaternion</h3><ul><li><p>The quaternion is an extension of the complex number – a hyper-complex number – and is written as a scalar plus a vector</p><blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/52.png" alt=""><br><img src="/2019/03/25/Notes-of-Manipulation-class/53.png" alt=""></p></blockquote></li><li><p>Coding part：<code>Quaternion()</code> is a class</p><blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/54.png" alt=""><img src="/2019/03/25/Notes-of-Manipulation-class/55.png" alt=""><img src="/2019/03/25/Notes-of-Manipulation-class/56.png" alt=""></p></blockquote></li></ul><h2 id="Combining-Translation-and-Orientation"><a href="#Combining-Translation-and-Orientation" class="headerlink" title="Combining Translation and Orientation"></a>Combining Translation and Orientation</h2><h3 id="Representing-pose"><a href="#Representing-pose" class="headerlink" title="Representing pose"></a>Representing pose</h3><ul><li>Pose<blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/57.png" alt=""></p></blockquote></li></ul><h3 id="Vecotr-Quaternion"><a href="#Vecotr-Quaternion" class="headerlink" title="Vecotr-Quaternion"></a>Vecotr-Quaternion</h3><ul><li>Vecotr-Quaternion<blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/58.png" alt=""></p></blockquote></li></ul><h3 id="homogeneous-transformation-matrix"><a href="#homogeneous-transformation-matrix" class="headerlink" title="homogeneous transformation matrix"></a>homogeneous transformation matrix</h3><ul><li><p>Form</p><blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/59.png" alt=""></p></blockquote></li><li><p>Properties</p><blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/60.png" alt=""></p></blockquote></li><li><p>Coding part：<code>transl(x,y,z)、trots(pi/20)</code>  <img src="/2019/03/25/Notes-of-Manipulation-class/62.png" alt=""></p></li></ul><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ul><li>Warpping up<blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/63.png" alt=""><img src="/2019/03/25/Notes-of-Manipulation-class/64.png" alt=""></p></blockquote></li></ul><h2 id="Coding-part"><a href="#Coding-part" class="headerlink" title="Coding part"></a>Coding part</h2><ul><li><p><code>rotx()</code>：3 dimensional rotation matrix</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ R = rotx(pi/2)</span><br><span class="line">$ R = rotx(45, &apos;deg&apos;)</span><br></pre></td></tr></table></figure></li><li><p><code>trplot() &amp;&amp; tranimate()</code>：display a 3d transformation</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ trplot(R)</span><br><span class="line">$ tranimate(R)</span><br></pre></td></tr></table></figure></li></ul><h1 id="Time-and-Motion"><a href="#Time-and-Motion" class="headerlink" title="Time and Motion"></a>Time and Motion</h1><h2 id="Trajectories"><a href="#Trajectories" class="headerlink" title="Trajectories"></a>Trajectories</h2><ul><li>An important characteristic of a trajectory is that is <strong>smooth</strong> – position and orientation vary smoothly with time  <blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/36.png" alt=""></p></blockquote></li></ul><h3 id="Smooth-One-Dimensional-Trajectories"><a href="#Smooth-One-Dimensional-Trajectories" class="headerlink" title="Smooth One-Dimensional Trajectories"></a>Smooth One-Dimensional Trajectories</h3><ul><li><p>Polynomial function of time    </p><blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/37.png" alt=""><br><img src="/2019/03/25/Notes-of-Manipulation-class/46.png" alt=""></p></blockquote></li><li><p><code>tpoly()</code>: generates a quintic polynomial trajectory    </p><blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/38.png" alt=""></p></blockquote><blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/39.png" alt=""></p></blockquote></li><li><p><code>lspb</code>：linear segment (constant velocity) with parabolic blends    </p><blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/40.png" alt=""><img src="/2019/03/25/Notes-of-Manipulation-class/41.png" alt=""></p></blockquote></li></ul><h3 id="Multi-Dimensional-Case"><a href="#Multi-Dimensional-Case" class="headerlink" title="Multi-Dimensional Case"></a>Multi-Dimensional Case</h3><ul><li><code>mtraj()/jtraj()</code>:  extend the smooth scalar trajectory to the vector case(多维)    <blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/42.png" alt=""></p></blockquote></li></ul><h3 id="Multi-Segment-Trajectories"><a href="#Multi-Segment-Trajectories" class="headerlink" title="Multi-Segment Trajectories"></a>Multi-Segment Trajectories</h3><ul><li><p>In robotics applications there is often a need to move smoothly along a path through one or more intermediate or via points without stopping. This might be to avoid obstacles in the workplace, or to perform a task that involves following a piecewise continuous trajectory.  </p><blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/43.png" alt="">   </p></blockquote></li><li><p><code>mstraj()</code>： generates a multi-segment multi-axis trajectory based on a matrix of via points    </p><blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/44.png" alt=""><img src="/2019/03/25/Notes-of-Manipulation-class/45.png" alt=""></p></blockquote></li></ul><h3 id="Interpolation-of-Orientation-in-3D"><a href="#Interpolation-of-Orientation-in-3D" class="headerlink" title="Interpolation of Orientation in 3D"></a>Interpolation of Orientation in 3D</h3><ul><li>A rotation matrix must be an orthogonal matrix     <blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/65.png" alt=""><img src="/2019/03/25/Notes-of-Manipulation-class/66.png" alt=""><img src="/2019/03/25/Notes-of-Manipulation-class/75.png" alt=""></p></blockquote></li><li>Quaternion interpolation     <blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/69.png" alt=""></p></blockquote></li></ul><ul><li><p><code>jtraj()/mtraj()</code>： roll-pitch-yaw angles can be interpolated    </p><blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/67.png" alt=""></p></blockquote></li><li><p><code>interp()</code>    </p><blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/68.png" alt=""><img src="/2019/03/25/Notes-of-Manipulation-class/70.png" alt=""></p></blockquote></li></ul><h3 id="Cartesian-Motion"><a href="#Cartesian-Motion" class="headerlink" title="Cartesian Motion"></a>Cartesian Motion</h3><blockquote><p>Another common requirement is a smooth path between two poses in SE(3) which involves change in position as well as in orientation. In robotics this is often referred to as Cartesian motion.<br><img src="/2019/03/25/Notes-of-Manipulation-class/71.png" alt=""></p></blockquote><ul><li>Coding part    2019/4/13 10:36:21 2019/4/13 10:36:24 <blockquote><p><img src="/2019/03/25/Notes-of-Manipulation-class/72.png" alt=""><img src="/2019/03/25/Notes-of-Manipulation-class/73.png" alt=""><img src="/2019/03/25/Notes-of-Manipulation-class/74.png" alt=""></p></blockquote></li></ul>]]></content:encoded>
      
      <comments>http://yoursite.com/2019/03/25/Notes-of-Manipulation-class/#disqus_thread</comments>
    </item>
    
    <item>
      <title>RNN&amp;&amp;LSTM</title>
      <link>http://yoursite.com/2019/03/24/DL_RNN-LSTM/</link>
      <guid>http://yoursite.com/2019/03/24/DL_RNN-LSTM/</guid>
      <pubDate>Sun, 24 Mar 2019 02:11:42 GMT</pubDate>
      <description>
      
        
        
          &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
        
      
      </description>
      
      <content:encoded><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a><a href="https://pytorch.org/docs/stable/nn.html#rnn" target="_blank" rel="noopener">RNN</a></h1><h2 id="理解RNN内部"><a href="#理解RNN内部" class="headerlink" title="理解RNN内部"></a>理解RNN内部</h2><ul><li><p><strong>我们可以用前馈神经网络（FFNNs）那样去理解</strong><img src="/2019/03/24/DL_RNN-LSTM/RNN-LSTM/3.png" alt=""><img src="/2019/03/24/DL_RNN-LSTM/RNN-LSTM/1.png" alt=""><img src="/2019/03/24/DL_RNN-LSTM/RNN-LSTM/2.png" alt=""></p></li><li><p><strong>Backpropagation through time(BPTT)</strong><img src="/2019/03/24/DL_RNN-LSTM/RNN-LSTM/4.png" alt=""><img src="/2019/03/24/DL_RNN-LSTM/RNN-LSTM/5.png" alt=""><img src="/2019/03/24/DL_RNN-LSTM/RNN-LSTM/6.png" alt=""></p><blockquote><p>For n-layers, can check <a href="https://classroom.udacity.com/nanodegrees/nd101/parts/0d85c39f-2ac0-49b3-98b0-fd40e9180cf4/modules/dd68b0bb-dd81-436f-bb04-e2a34ed349b8/lessons/74236975-4329-4704-9890-85b51f3f35fa/concepts/0f22f887-59e5-4bac-a2e9-d85c4f4b1c69" target="_blank" rel="noopener">here</a>  </p></blockquote></li></ul><h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><h1 id="Word-Embeddings"><a href="#Word-Embeddings" class="headerlink" title="Word Embeddings"></a>Word Embeddings</h1><h2 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h2><ul><li>Reduce the dimensionality of text data.  </li><li>Learn some interesting traits about words in a vocabulary.   <blockquote><p><img src="/2019/03/24/DL_RNN-LSTM/RNN-LSTM/7.png" alt=""></p></blockquote></li></ul><h2 id="Embedding-Weight-Matrix-Lookup-table"><a href="#Embedding-Weight-Matrix-Lookup-table" class="headerlink" title="Embedding Weight Matrix/Lookup table"></a>Embedding Weight Matrix/Lookup table</h2><ul><li><p>The embedding can graetly improve the ability of networks to learn from text data, by representing that data as lower-dimensional vectors.  </p><blockquote><p><img src="/2019/03/24/DL_RNN-LSTM/RNN-LSTM/8.png" alt=""><br><img src="/2019/03/24/DL_RNN-LSTM/RNN-LSTM/9.png" alt=""><img src="/2019/03/24/DL_RNN-LSTM/RNN-LSTM/10.png" alt=""></p></blockquote></li><li><p>Embedding lookup    </p><blockquote><p><img src="/2019/03/24/DL_RNN-LSTM/RNN-LSTM/11.png" alt=""></p></blockquote></li></ul><h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><blockquote><p>The Word2Vec algorithm finds much more efficient representations by finding vectors that represent the words. These vectors also contain semantic information about the words.</p></blockquote><ul><li><strong>Two architectures for implementing Word2Vec:</strong>    <blockquote><p><img src="/2019/03/24/DL_RNN-LSTM/RNN-LSTM/12.png" alt=""></p></blockquote></li></ul><h1 id="CODING"><a href="#CODING" class="headerlink" title="CODING"></a>CODING</h1><ol><li>将数据feed给FC层之前，一般需要进行<em>“降维”</em><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># shape output to be (batch_size*seq_length, hidden_dim)</span><br><span class="line">r_out = r_out.view(-1, self.hidden_dim)  </span><br><span class="line"># sometime you may need to use contiguous to reshape the output</span><br><span class="line">r_out = r_out.contiguous().view(-1, self.hidden_dim)</span><br></pre></td></tr></table></figure></li></ol>]]></content:encoded>
      
      <comments>http://yoursite.com/2019/03/24/DL_RNN-LSTM/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Data processing with numpy or pytorch</title>
      <link>http://yoursite.com/2019/03/09/numpy-and-pytorch-in-coding/</link>
      <guid>http://yoursite.com/2019/03/09/numpy-and-pytorch-in-coding/</guid>
      <pubDate>Sat, 09 Mar 2019 04:19:03 GMT</pubDate>
      <description>
      
        
        
          &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
        
      
      </description>
      
      <content:encoded><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Numpy"><a href="#Numpy" class="headerlink" title="Numpy"></a>Numpy</h1><h2 id="Numpy从数值范围创建数组：arange-、linspace-、logsapce"><a href="#Numpy从数值范围创建数组：arange-、linspace-、logsapce" class="headerlink" title="Numpy从数值范围创建数组：arange()、linspace()、logsapce()"></a><a href="http://www.runoob.com/numpy/numpy-array-from-numerical-ranges.html" target="_blank" rel="noopener">Numpy从数值范围创建数组：arange()、linspace()、logsapce()</a></h2><ul><li><code>numpy.arange(start=0, stop, step=1, dtype)</code>:  start 与 stop 指定的范围以及 step 设定的步长，生成一个 ndarray,不包含<code>stop</code>值；  </li><li><code>np.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None)</code>:    创建一个等差数列构成的一维数组，<code>num</code>为样本量；  </li><li><code>np.logspace(start, stop, num=50, endpoint=True, base=10.0, dtype=None)</code>: 创建一个等比数列构成的一维数组，<code>base</code>为底；</li></ul><h1 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h1><h2 id="pytorch中torch-view-、squeeze-、unsqueeze-、max"><a href="#pytorch中torch-view-、squeeze-、unsqueeze-、max" class="headerlink" title="pytorch中torch.view()、squeeze()、unsqueeze()、max()"></a><a href="https://blog.csdn.net/lanse_zhicheng/article/details/79148678#commentBox" target="_blank" rel="noopener">pytorch中torch.view()、squeeze()、unsqueeze()、max()</a></h2><p>numel()<br>permute()<br>tqdmfire网络参数初始化问题<a href="https://cloud.tencent.com/developer/ask/51836" target="_blank" rel="noopener">meshgrid()</a>stackravelnp.newaxiswhereastype()</p><p>numpy.prob()连乘assert</p><p>a.shape=(1,4,4)a[:,:,0::4].shape=(1,4,1)而a[:,:,0].shape=(1,4)</p><p>if set model.eval() ——&gt; model.training = Flase</p><p><a href="https://blog.csdn.net/weixin_41010198/article/details/84828022" target="_blank" rel="noopener">python类方法中使用：修饰符@staticmethod和@classmethod的作用与区别，还有装饰器@property的使用</a></p><p><a href="https://blog.csdn.net/u011501388/article/details/84062483" target="_blank" rel="noopener">PyTorch之前向传播函数forward</a></p><p>open.cv读入图片的尺寸是（H,W,C）</p><p>Input type(torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be he same:::原因可能是model放进了GPU而输入数据没有放到GPU。</p><p>expected object of backend CPU but got backend CUDA    for sequence element 1 in sequence argument at position #1 ‘tensors’:CUDA与CPU的数据不能混在一起运算，不会自由转换，<a href="https://www.zhihu.com/question/67209417" target="_blank" rel="noopener">门</a></p><p>when set model.eval(), self.training/model.training = False?</p><p><a href="https://discuss.pytorch.org/t/torch-from-numpy-not-support-negative-strides/3663" target="_blank" rel="noopener">ValueError: some of the strides of a given numpy array are negative. This is currently not supported, but will be added in future releases.</a> </p>]]></content:encoded>
      
      <comments>http://yoursite.com/2019/03/09/numpy-and-pytorch-in-coding/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Deep learning in Pytorch</title>
      <link>http://yoursite.com/2019/03/05/pytorch%EF%BC%9ADeeplearning-in-Pytorch-Udacity/</link>
      <guid>http://yoursite.com/2019/03/05/pytorch%EF%BC%9ADeeplearning-in-Pytorch-Udacity/</guid>
      <pubDate>Tue, 05 Mar 2019 11:40:24 GMT</pubDate>
      <description>
      
        
        
          &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla
        
      
      </description>
      
      <content:encoded><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Architectures"><a href="#Architectures" class="headerlink" title="Architectures"></a>Architectures</h1><h1 id="Utility-tool"><a href="#Utility-tool" class="headerlink" title="Utility tool"></a>Utility tool</h1><h2 id="check-weights-or-bias"><a href="#check-weights-or-bias" class="headerlink" title="check weights or bias"></a>check weights or bias</h2><ol><li>通过<em>print</em> <code>model.state_dict()</code>或<code>model.named_parameters()</code>查看所有参数，信息包括<code>name</code>和<code>params</code>，然后使用点运算符<code>.</code>查看相应的参数；</li><li><p>通过官网查看相应网络模块具有的<code>variables</code>，然后使用点运算符<code>.</code>查看相应的参数； </p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model.fc.weight</span><br><span class="line">model.fc.bias</span><br><span class="line">model.rnn.weight_ih_l0</span><br><span class="line">model.rnn.bias_hh_l1</span><br></pre></td></tr></table></figure></li></ol><h2 id="Running-with-GPU"><a href="#Running-with-GPU" class="headerlink" title="Running with GPU"></a>Running with GPU</h2><ul><li><p>查看GPU是否可用，然后使用<code>.to()</code>使用相应device：</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure></li><li><p>常用GPU命令</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.is_available()</span><br><span class="line">cuda是否可用；</span><br><span class="line"></span><br><span class="line">torch.cuda.device_count()</span><br><span class="line">返回gpu数量；</span><br><span class="line"></span><br><span class="line">torch.cuda.get_device_name(0)</span><br><span class="line">返回gpu名字，设备索引默认从0开始；</span><br><span class="line"></span><br><span class="line">torch.cuda.current_device()</span><br><span class="line">返回当前设备索引；</span><br></pre></td></tr></table></figure></li></ul><h2 id="保存或加载模型"><a href="#保存或加载模型" class="headerlink" title="保存或加载模型"></a>保存或加载模型</h2><ul><li><p>快速保存模型：</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), &apos;path/file_name.pth&apos;)</span><br></pre></td></tr></table></figure></li><li><p>详细保存：包括其他一下信息</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">checkpoints=&#123;&apos;label&apos;: model.n_label,</span><br><span class="line"> &apos;input_size&apos;: model.input_size,</span><br><span class="line"> &apos;state_dict&apos;: model.state_dict()&#125;</span><br><span class="line">with open(os.path.join(args.save+fold,model_name), &apos;wb&apos;) as f:</span><br><span class="line">torch.save(checkpoint, f)</span><br></pre></td></tr></table></figure></li><li><p>加载：</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict((torch.load(path/file_name.pth)))</span><br></pre></td></tr></table></figure></li></ul><h2 id="迁移学习：torchvision-models"><a href="#迁移学习：torchvision-models" class="headerlink" title="迁移学习：torchvision.models"></a>迁移学习：<code>torchvision.models</code></h2><ul><li><p>直接从<code>torchvision</code>模块里加载模型：</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import torchvision.models as models/ from torchvision import models</span><br><span class="line">model = models.vgg16(pretrained=True/ not args.caffe_pretrain)</span><br></pre></td></tr></table></figure></li><li><p>freeze parameters1</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">for param in model.features.parameters():</span><br><span class="line">param.require_grad = False</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line">- freeze specified parameter2</span><br></pre></td></tr></table></figure><h1 id="delete-the-last-MaxPooling-layer"><a href="#delete-the-last-MaxPooling-layer" class="headerlink" title="delete the last MaxPooling layer"></a>delete the last MaxPooling layer</h1><h1 id="freeze-the-first-4-convs"><a href="#freeze-the-first-4-convs" class="headerlink" title="freeze the first 4 convs"></a>freeze the first 4 convs</h1><p>  features = list(model.features)[:30]   for layer in features[:10]: </p><pre><code>for p in layer.parameters():     p.requires_grad = False </code></pre><p>  features = nn.Sequential(*features)</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&gt; 最好检查一下是否“冻结”了参数，必要时在`optimizer`使用`filter`过滤一遍,[传送门][filter]：   </span><br><span class="line">&gt; `optimizer.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)`</span><br><span class="line"></span><br><span class="line">- 删除或增加layer：转成`list`方便操作，后面再使用`nn.Sequential(*list)`构建模型，其中的`*`是将列表或元组（可能适合其他类型）数据拆分；</span><br><span class="line"></span><br><span class="line">``` </span><br><span class="line"># delete the last Linear layer </span><br><span class="line">classifier = list(model.classifier) </span><br><span class="line">in_feature = classifier[6].in_feature</span><br><span class="line">del classifier[6] </span><br><span class="line">if not args.use_drop: </span><br><span class="line">del classifier[2] </span><br><span class="line">del classifier[5] </span><br><span class="line">classifier = nn.Sequential(*classifier) </span><br><span class="line">classifier.add_module(str(6), nn.Linear(in_feature, 10))</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># python #</span><br><span class="line">## 使用[os模块][os]创建文件夹 ##</span><br><span class="line">&gt; 一般先使用`os.path.exists(path)`来检查相应文件夹是否存在</span><br><span class="line">1. 使用`os.mkdir(path)`创建目录；</span><br><span class="line">2. 使用`os.makedirs(path)`递归创建目录；</span><br></pre></td></tr></table></figure><p>  if not os.path.exists(path):</p><pre><code>os.makedirs(path)</code></pre><p>  <code>`</code></p></li></ul>]]></content:encoded>
      
      <comments>http://yoursite.com/2019/03/05/pytorch%EF%BC%9ADeeplearning-in-Pytorch-Udacity/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
